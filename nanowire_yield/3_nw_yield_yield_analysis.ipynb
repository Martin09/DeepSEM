{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_nw_yield-yield_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Martin09/DeepSEM/blob/master/nanowire_yield/3_nw_yield_yield_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SMEbJUHYJh9",
        "colab_type": "text"
      },
      "source": [
        "# 3 - Model loading and NW yield analysis\n",
        "In this notebook we will:\n",
        "1. Load an image for analysis.\n",
        "2. Load our previously-trained model.\n",
        "3. Use model to label the SEM image.\n",
        "4. Perform post-processing on model output to calculate the NW yield\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM54r6jlKTII",
        "colab_type": "text"
      },
      "source": [
        "## Install detectron2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_FzH13EjseR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install dependencies: (use cu101 because colab has CUDA 10.1)\n",
        "!pip install -U torch==1.5 torchvision==0.6 -f https://download.pytorch.org/whl/cu101/torch_stable.html \n",
        "!pip install cython pyyaml==5.1\n",
        "!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
        "import torch, torchvision\n",
        "print(torch.__version__, torch.cuda.is_available())\n",
        "!gcc --version"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-i4hmGYk1dL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install detectron2:\n",
        "!pip install detectron2==0.1.2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/index.html"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyAvNCJMmvFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "from glob import glob\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGJYcrPIRE3v",
        "colab_type": "text"
      },
      "source": [
        "## 3.1 - Unpack and load our image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kufCnL8z-vZo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clone the DeepSEM repository\n",
        "!rm -rf DeepSEM\n",
        "!git clone https://github.com/Martin09/DeepSEM\n",
        "\n",
        "# OR\n",
        "\n",
        "# # Load from GoogleDrive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive/')\n",
        "# %cd \"/content/gdrive/My Drive/path/to/your/data\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8N7u_n7RLwZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_dir = './DeepSEM/nanowire_yield/datasets/NWs-8kMag-rawtiffs/'\n",
        "!wget https://github.com/Martin09/DeepSEM/raw/master/nanowire_yield/datasets/NWs-8kMag-rawtiffs.zip\n",
        "!unzip NWs-8kMag-rawtiffs.zip -d $dataset_dir\n",
        "!rm NWs-8kMag-rawtiffs.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2KM5HQp4gHh",
        "colab_type": "text"
      },
      "source": [
        "Load an image and show it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuFUcV5tq8Tv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# im = cv2.imread(\"./DeepSEM/nanowire_yield/datasets/NWs-8kMag-subdivided/135nm_20_cropped_x0_y0.png\", cv2.IMREAD_GRAYSCALE)\n",
        "im = cv2.imread(\"./DeepSEM/nanowire_yield/datasets/NWs-8kMag-rawtiffs/140nm_16.tif\", cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "print(im.shape)\n",
        "cv2_imshow(im)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szbPuPoKOKJR",
        "colab_type": "text"
      },
      "source": [
        "Do some pre-processing to get it ready to feed into our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYDE7vVMqq78",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model expects an RGB image, so copy the greyscale data into other 2 channels\n",
        "im_RGB = np.repeat(im[:, :, np.newaxis], 3, axis=2)\n",
        "print(im_RGB.shape)\n",
        "\n",
        "# Trim off the bottom overlay to not confuse the model (if needed)\n",
        "im_RGB = im_RGB[:688,:,:] \n",
        "print(im_RGB.shape)\n",
        "cv2_imshow(im_RGB)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDEx09lYVyYE",
        "colab_type": "text"
      },
      "source": [
        "## 3.2 - Load our model\n",
        "\n",
        "Now we will load a trained model and use it to label the above image. First we load a default config with `get_cfg()` and we then overwrite some of its parameters with our saved YAML configuration file. \n",
        "\n",
        "One important point is that we need to have `cfg.MODEL.WEIGHTS` set to point to the weights file. As this file can be quite big (>300MB) and since Github isn't designed to host big binary files, I have saved the weights for this model on my Google Drive instead. However, if you have your weights saved locally (ex: on your Google Drive), you can skip these next couple of lines."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBQbpIgXTi9D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "file_id = '1K_XnLc69uA2FRSdIFPs-WvTSrAXKwve2'\n",
        "weights_file = './DeepSEM/nanowire_yield/trained_models/nw_yield_it10k_loss0.133.pth'\n",
        "if os.path.isfile(weights_file):\n",
        "  print('Weights file already exists. Skipping download!')\n",
        "else:\n",
        "  !gdown --id $file_id -O $weights_file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRf9lQoyeIGg",
        "colab_type": "text"
      },
      "source": [
        "Now we can go ahead with the rest of the configuration of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLjc8j5wY7Yr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from detectron2.config import get_cfg\n",
        "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
        "\n",
        "model_path = './DeepSEM/nanowire_yield/trained_models/nw_yield_it10k_loss0.133.yaml'\n",
        "# model_path = './DeepSEM/nanowire_yield/trained_models/nw_yield_it99_loss0.93.yaml'\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_path)\n",
        "cfg.MODEL.DEVICE = 'cpu'  # CPU is enough for inference, no need for GPU\n",
        "\n",
        "# If we have a lot of objects to detect, need to set higher # of proposals here:\n",
        "cfg.MODEL.RPN.POST_NMS_TOPK_TEST = 10000\n",
        "cfg.MODEL.RPN.PRE_NMS_TOPK_TEST = 10000\n",
        "cfg.TEST.DETECTIONS_PER_IMAGE = 10000\n",
        "\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5   # Set the testing threshold for this model\n",
        "cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = 0.2     # Non-max supression threshold\n",
        "\n",
        "# Setting allowed input sizes (avoid scaling)\n",
        "cfg.INPUT.MIN_SIZE_TEST = 0\n",
        "cfg.INPUT.MAX_SIZE_TEST = 99999\n",
        "\n",
        "# A bit of a hacky way to be able to use the DefaultPredictor:\n",
        "# Register a \"fake\" dataset to then set the 'thing_classes' metadata\n",
        "# (there is probably a better way to do this...)\n",
        "cfg.DATASETS.TEST = ('placeholder')\n",
        "DatasetCatalog.clear()\n",
        "DatasetCatalog.register(\"placeholder\", lambda _: None)\n",
        "MetadataCatalog.get(\"placeholder\").set(thing_classes=[\"nw\",\"parasitic\",\"nanospade\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQhs7Ajn4qnF",
        "colab_type": "text"
      },
      "source": [
        "## 3.3 - Performing inference\n",
        "Create a predictor, and run the predictor on the image to generate the outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQbplWgJiaMq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictor = DefaultPredictor(cfg)\n",
        "outputs = predictor(im_RGB)\n",
        "print('Number of detected objects = {}'.format(len(outputs[\"instances\"])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xtRLxUNrA-X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Verify outputs manually\n",
        "# outputs[\"instances\"].pred_classes\n",
        "# outputs[\"instances\"].pred_boxes\n",
        "# outputs[\"instances\"].scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oA8D3N-J45Lh",
        "colab_type": "text"
      },
      "source": [
        "Now we can visualize the output of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ye6rOKIhrFgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We can use Visualizer to draw the predictions on the image.\n",
        "v = Visualizer(im_RGB[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TEST[0]), scale=2.0)\n",
        "v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "cv2_imshow(v.get_image()[:, :, ::-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-QFwAFgefc-",
        "colab_type": "text"
      },
      "source": [
        "Hopefully you should see that almost all the objects in the full-scale SEM image have now been labelled. We can also play a bit with the `SCORE_THRESH_TEST` and `NMS_THRESH_TEST` to fine-tune the output of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20XCitnL5F42",
        "colab_type": "text"
      },
      "source": [
        "## 3.4 - Post-processing model output\n",
        "\n",
        "However, just getting the output from the model isn't enough. Now we have to do quite a bit of work to post-process the output and extract a nanowire yield from this. Let's do this!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlAMwi-Rocdr",
        "colab_type": "text"
      },
      "source": [
        "Massage the output a bit to get it into a single numpy array for easy filtering."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXZYiByPmX_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cl = np.array(outputs[\"instances\"].pred_classes.cpu())  # Classes\n",
        "s = np.array(outputs[\"instances\"].scores.cpu()) # Prediction scores\n",
        "b =  np.array([x.numpy() for x in outputs[\"instances\"].pred_boxes])  # Bounding boxes\n",
        "c = np.array(outputs[\"instances\"].pred_boxes.get_centers())  # Bounding box centres\n",
        "predictions = np.column_stack([cl,s,b,c])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubv36M6-m38f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Select only NW predictions\n",
        "predictions = predictions[predictions[:,0]==0]\n",
        "\n",
        "# Extract NW bounding box centres\n",
        "centres = predictions[:,-2:]\n",
        "C = centres  # Tensor variable notation to use later in matrix calculations\n",
        "\n",
        "# Loop over NW bounding box centres. For each one, find vector for 3 closest NWs\n",
        "vectors = []\n",
        "for p in centres:\n",
        "\n",
        "    # Build a tensor as large as centres tensor\n",
        "    p = p[np.newaxis]\n",
        "    P = np.repeat(p, len(C), axis=0)\n",
        "\n",
        "    # Calculate distance to all other NWs in a vectorized way\n",
        "    D = np.linalg.norm(C - P, axis=1)\n",
        "    n = 3 # Number of minimum distances to extract\n",
        "    i_min = np.argsort(-D)[-1-n:-1]  # Find \"n\" minimum distances\n",
        "    V = C[i_min] - np.repeat(p, n, axis=0)\n",
        "    vectors.extend(V.squeeze().tolist())\n",
        "\n",
        "vectors = np.array(vectors)\n",
        "lengths = np.linalg.norm(vectors, axis=1)\n",
        "\n",
        "plt.plot(vectors[:,0], vectors[:,1],'.')\n",
        "plt.grid()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yobjuE9pqyjT",
        "colab_type": "text"
      },
      "source": [
        "Above we have a plot of the nanowire grid from which we can now extract the grid vectors. First, however, we will do some outlier removal with scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84emeydYnKQ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "\n",
        "X = vectors\n",
        "\n",
        "# Fit model for local outlier function\n",
        "clf = LocalOutlierFactor(n_neighbors=25, contamination=0.1, algorithm='auto')\n",
        "_ = clf.fit_predict(X)  # Make prediction of outliers\n",
        "X_scores = clf.negative_outlier_factor_  # Get outlier scores (lower score means more of an outlier)\n",
        "\n",
        "# Plot outlier analysis\n",
        "plt.title(\"Local Outlier Factor (LOF)\")\n",
        "plt.scatter(X[:, 0], X[:, 1], color='k', s=3., label='Data points')\n",
        "\n",
        "radius = (X_scores.max() - X_scores) / (X_scores.max() - X_scores.min())\n",
        "plt.scatter(X[:, 0], X[:, 1], s=1000 * radius, edgecolors='r',\n",
        "            facecolors='none', label='Outlier scores')\n",
        "plt.grid()\n",
        "legend = plt.legend(loc='upper left')\n",
        "legend.legendHandles[0]._sizes = [10]\n",
        "legend.legendHandles[1]._sizes = [20]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBsVP153v5EQ",
        "colab_type": "text"
      },
      "source": [
        "Can now filter the points based on their \"outlier scores\" to get only the grid vectors that we are interested in."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aT-jL2Vqrsb_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_filt = X[X_scores[:] > -1.5,:]  # Filter points based on outlier scores\n",
        "plt.scatter(X_filt[:, 0], X_filt[:, 1], color='k')\n",
        "plt.grid()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQmba0EmQnV1",
        "colab_type": "text"
      },
      "source": [
        "We have many point groups now, we just need to find those that correspond to the unit vectors (shortest). So we apply an additional step of filtering and extract only the shortest vectors by plotting a histogram."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lReJQQwNEjuo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "D = np.linalg.norm(X_filt,axis=1)\n",
        "n, bins, _ = plt.hist(D)\n",
        "\n",
        "i_max = np.argsort(n)[-1]  # Find index of maximum\n",
        "i_filt = list(n).index(0,i_max)  # After this maximum, find first index where N=0\n",
        "\n",
        "# Filter out all vectors that are longer than this\n",
        "D_f = bins[i_filt]\n",
        "plt.axvline(x=D_f,ls='--',color='r') # Plot red line to show where filter cut-off is\n",
        "X_filt2 = X_filt[np.linalg.norm(X_filt,axis=1) < D_f]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZWu4tBKwanT",
        "colab_type": "text"
      },
      "source": [
        "Apply k-means clustering to the filtered list to extract the remaining four points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYlRZ0F0npNV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans = KMeans(n_clusters=4, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
        "pred_y = kmeans.fit_predict(X_filt2)\n",
        "plt.scatter(X_filt2[:,0], X_filt2[:,1])\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')\n",
        "plt.grid()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFoTnsjvzrCu",
        "colab_type": "text"
      },
      "source": [
        "Convert those 4 points into the two grid vectors that we need.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gP-1RxfTzj5A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tmp_pos = [x for x in kmeans.cluster_centers_ if x[0] > 0 and x[1] > 0][0]\n",
        "tmp_neg = [x for x in kmeans.cluster_centers_ if x[0] < 0 and x[1] < 0][0]\n",
        "v1 = np.mean([tmp_pos,-tmp_neg],axis=0)\n",
        "\n",
        "tmp_pos = [x for x in kmeans.cluster_centers_ if x[0] > 0 and x[1] < 0][0]\n",
        "tmp_neg = [x for x in kmeans.cluster_centers_ if x[0] < 0 and x[1] > 0][0]\n",
        "v2 = np.mean([tmp_pos,-tmp_neg],axis=0)\n",
        "\n",
        "# Vectors v1 and v2 now characterize the grid on which the NWs appear\n",
        "print(f'Vector 1 = {v1}')\n",
        "print(f'Vector 2 = {v2}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP6AU2X47_eZ",
        "colab_type": "text"
      },
      "source": [
        "Now we can use these vectors to generate an array of points where we expect to find the NWs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDEmbJtJrJBP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Determine how many grid points (roughly) we need to generate to cover this image\n",
        "image_width, image_height = im_RGB.shape[:2]\n",
        "l_min = np.min([np.linalg.norm(v1),np.linalg.norm(v2)])\n",
        "n = int(np.max([image_height,image_width])/l_min)+1 # Round up, to be safe\n",
        "\n",
        "# Generate array of grid points where to look for NWs\n",
        "grid_pts = np.array(np.meshgrid(range(-n,n), range(-n,n))).T.reshape(-1,2)  # Create standard grid\n",
        "V = np.column_stack([v1,v2])\n",
        "grid_pts = grid_pts.dot(V.T) # Coordinate transformation\n",
        "grid_pts += np.array([image_width, image_height])/2 # Shift grid to centre of image\n",
        "\n",
        "# Remove points outside image\n",
        "b = np.max([np.linalg.norm(v1),np.linalg.norm(v2)])*0.8 # Area around edges of image\n",
        "grid_pts = np.array([[x,y] for x,y in grid_pts if (0+b<x<image_height-b) and (0+b<y<image_width-b)])\n",
        "\n",
        "# Plot both the grid points and the NW centres together to see if they more or less match\n",
        "plt.plot(centres[:,0],centres[:,1],'b.')\n",
        "plt.plot(grid_pts[:,0],grid_pts[:,1],'r.')\n",
        "plt.grid()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Se2VKTa3Utj",
        "colab_type": "text"
      },
      "source": [
        "Now we have the grid points, but they are not perfectly aligned to the NW positions, as seen above. Therefore perform some optimization with scipy to minimize the error between the grid points and the NW points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3Z1yyUs2Qsz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import leastsq, minimize\n",
        "\n",
        "def get_grid(grid_pts, shift):\n",
        "    return grid_pts + np.array(shift)\n",
        "\n",
        "def get_mean_squared_distance(nw_pts, g_pts):\n",
        "    err_vector = np.array([0.,0.])\n",
        "    for p in nw_pts:\n",
        "        D = np.linalg.norm(g_pts - p, axis=1)\n",
        "        err_vector += (g_pts[np.argmin(D)] - p)**2\n",
        "        # err_vector += (g_pts[np.argmin(D)] - p)\n",
        "    return np.linalg.norm(err_vector)\n",
        "\n",
        "def err_func(params, nw_centres, grid_pts):\n",
        "    shifted_grid = get_grid(grid_pts, params)\n",
        "    err = get_mean_squared_distance(nw_centres, shifted_grid)\n",
        "    return err\n",
        "\n",
        "# Find the optimal shift of the grid to minimize error vector for all points\n",
        "out = minimize(err_func, \n",
        "               x0=(4, 5),\n",
        "               method='Nelder-Mead', \n",
        "               args=(centres, grid_pts))    \n",
        "out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wk4l2aW3q0n",
        "colab_type": "text"
      },
      "source": [
        "out.x gives us the optimized shift that minimizes the error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKdQmiVR2X6O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apply the optimized shift to the grid and plot it\n",
        "new_grid_pts = grid_pts + out.x\n",
        "plt.plot(centres[:,0],centres[:,1],'b.')\n",
        "plt.plot(new_grid_pts[:,0],new_grid_pts[:,1],'r.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptsitVZ732ST",
        "colab_type": "text"
      },
      "source": [
        "Now that the point grid and nanowire grid are aligned, can now perform the yield analysis. This is easiest if we transform the coordinates into the grid vector space. This is just the inverse of the transformation matrix that we already created to generate the grid points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz6VnLfV2bbs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "V_inv = np.linalg.inv(V) # Inverse coordinate transformation matrix\n",
        "\n",
        "# Bring the NW center points back into grid vector space\n",
        "centres_vcoords = centres.dot(V_inv.T)\n",
        "grid_pts_vcoords = new_grid_pts.dot(V_inv.T)\n",
        "\n",
        "# Plot them in grid vector space\n",
        "plt.plot(centres_vcoords[:,0], centres_vcoords[:,1],'b.')\n",
        "plt.plot(grid_pts_vcoords[:,0],grid_pts_vcoords[:,1],'r.')\n",
        "plt.grid()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NASQCaC9ATq",
        "colab_type": "text"
      },
      "source": [
        "Now we can loop over each grid point and look for a corresponding NW that is <0.5 grid vector units away in both x and y."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JahLMdcn2mu1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "has_nw_list = [] # Add here if there is a NW in this position or not\n",
        "patch_list = [] # Add here the coordinates of the patches for later plotting on top of the image\n",
        "# Loop over all grid points\n",
        "for pt in grid_pts_vcoords:\n",
        "    # Calculate distances from this point to all NW centres\n",
        "    D = np.linalg.norm(centres_vcoords - pt, axis=1)\n",
        "\n",
        "    # Find the closest NW centre\n",
        "    closest_nw = centres_vcoords[np.argmin(D)]\n",
        "    \n",
        "    # Is this NW within the current grid point unit cell?\n",
        "    has_nw = pt[0] - 0.5 < closest_nw[0] < pt[0] + 0.5 and pt[1] - 0.5 < closest_nw[1] < pt[1] + 0.5\n",
        "    has_nw_list.append(has_nw)\n",
        "\n",
        "    # Save the coordinates of this grid point unit cell for later plotting\n",
        "    patch = np.array([pt+[0.5,0.5],pt+[-0.5,0.5],pt+[-0.5,-0.5],pt+[0.5,-0.5]])\n",
        "    patch_list.append(patch)\n",
        "\n",
        "patch_list = np.array(patch_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbjKkZe3-CFk",
        "colab_type": "text"
      },
      "source": [
        "Now we have enough information to calculate NW yield"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EKCJLrb2tLL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nw_yield = has_nw_list.count(True)/len(has_nw_list)\n",
        "print(\"NW yield is {:.0f}/{:.0f} = {:.0f}%!\".format(has_nw_list.count(True), len(has_nw_list), nw_yield*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du-B4UfF-KRE",
        "colab_type": "text"
      },
      "source": [
        "To visualize this, transform the patches back into original image coordinates and plot the patches based on if there was a NW found in them or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TP3Ot0X2zQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transform from grid vector space back into image coordinate space\n",
        "patches_imgcoords = patch_list.reshape([-1,2]).dot(V.T).reshape([-1,4,2])\n",
        "\n",
        "# Plot the image\n",
        "plt.figure(figsize=(20,12))\n",
        "plt.imshow(im, cmap='gray', vmin=0, vmax=255)\n",
        "current_axis = plt.gca()\n",
        "\n",
        "# Plot semi-transparent patches on top\n",
        "colors = ['r', 'g']\n",
        "for i, patch in enumerate(patches_imgcoords):\n",
        "    color = colors[has_nw_list[i]]\n",
        "    current_axis.add_patch(plt.Polygon(patch, color=color, fill=True, linewidth=0, alpha = 0.4))  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQ5EAmzqlZ9A",
        "colab_type": "text"
      },
      "source": [
        "That's it! Hopefully the post-processing steps ran OK for you and you see a nice SEM image with a visual representation of how many NWs were grown compared to parasitic growths."
      ]
    }
  ]
}