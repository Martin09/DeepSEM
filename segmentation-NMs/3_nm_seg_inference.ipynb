{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NM Segmentation 3: Loading Trained Model and Inference",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Martin09/DeepSEM/blob/master/segmentation-NMs/3_nm_seg_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SMEbJUHYJh9",
        "colab_type": "text"
      },
      "source": [
        "# 3 - Model loading and NM size/yield analysis\n",
        "In this notebook we will:\n",
        "1. Load an image for analysis.\n",
        "2. Load our previously-trained model.\n",
        "3. Use model to label the SEM image.\n",
        "4. Perform post-processing on model output to learn about our NM characteristics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDmXb0wNOyri",
        "colab_type": "text"
      },
      "source": [
        "Note: A GPU instance is not necessary for this notebook as we will only be performing inference which is not as computationally-expensive as training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM54r6jlKTII",
        "colab_type": "text"
      },
      "source": [
        "## Install detectron2\n",
        "Again, we will be using Facebook's [detectron2](https://github.com/facebookresearch/detectron2) library to run the interence on our images to let's install it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_FzH13EjseR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install dependencies: (use cu101 because colab has CUDA 10.1)\n",
        "!pip install -U torch==1.5 torchvision==0.6 -f https://download.pytorch.org/whl/cu101/torch_stable.html \n",
        "!pip install cython pyyaml==5.1\n",
        "!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
        "import torch, torchvision\n",
        "print(torch.__version__, torch.cuda.is_available())\n",
        "!gcc --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-i4hmGYk1dL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install detectron2:\n",
        "!pip install detectron2==0.1.2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/index.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyAvNCJMmvFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger('logs')\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, cv2, random, tifffile, json, datetime, time, urllib\n",
        "from glob import glob\n",
        "from google.colab.patches import cv2_imshow\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import DatasetCatalog, MetadataCatalog"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7xOxlkQQ5LQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the classes in our dataset\n",
        "class_dict = {'slit': '1',\n",
        "              'nanomembrane': '2',\n",
        "              'parasitic': '3',\n",
        "              'bottom_nucleus': '4',\n",
        "              'side_nucleus': '5',\n",
        "              'nanowire': '6',\n",
        "              'overgrowth': '7'}\n",
        "\n",
        "# Define some paths/constants that will be useful later\n",
        "desired_mag = 50000  # Used to filter the TIFF input files\n",
        "\n",
        "root = Path('./DeepSEM/segmentation-NMs/')\n",
        "dataset_dir = root.joinpath('datasets')\n",
        "output_dir = root.joinpath('output')\n",
        "models_dir = root.joinpath('trained_models')\n",
        "\n",
        "imgs_zip = dataset_dir.joinpath('Nick_NMs_allrawimgs.zip')\n",
        "imgs_dir = dataset_dir.joinpath(imgs_zip.stem)\n",
        "imgs_google_drive_id = '1M2_0GLScsNY53w8hU2xJdXtisESfkOqI'\n",
        "\n",
        "test_dir = imgs_dir.joinpath('test')\n",
        "train_dir = imgs_dir.joinpath('train')\n",
        "\n",
        "dataset_root_name = 'nm_masks'\n",
        "train_name = dataset_root_name + '_train'\n",
        "test_name = dataset_root_name + '_test'\n",
        "\n",
        "# model_path = models_dir.joinpath('nm_seg_it20k_loss0.028.yaml')\n",
        "# weights_path = models_dir.joinpath('nm_seg_it20k_loss0.028.pth')\n",
        "\n",
        "model_path = models_dir.joinpath('nm_seg_it19999_lossX.XXX.yaml')\n",
        "weights_path = models_dir.joinpath('nm_seg_it19999_lossX.XXX.pth')\n",
        "\n",
        "weights_google_drive_id = '1btMy-EyU2sTSSPQk8kf663DYgO-sD3kR'\n",
        "# weights_google_drive_id = '1QDyirJCJlZwvuIKGfKRH0nCU1Dw_64G5'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGJYcrPIRE3v",
        "colab_type": "text"
      },
      "source": [
        "## 3.1 - Unpack and load our images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kufCnL8z-vZo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Optional: Save everything to your own GoogleDrive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive/')\n",
        "# %cd \"/content/gdrive/My Drive/path/to/save/location\"\n",
        "\n",
        "# Clone just the relevant folder from the DeepSEM repo\n",
        "!rm -rf $root\n",
        "!apt install subversion\n",
        "!svn checkout $github_url $root\n",
        "\n",
        "# # Alternative: Clone whole DeepSEM repository\n",
        "# !rm -rf DeepSEM  # Remove folder if it already exists\n",
        "# !git clone https://github.com/Martin09/DeepSEM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FF1vtvEJtjK",
        "colab_type": "text"
      },
      "source": [
        "For simplicity, I will use our previous training images for inference. However these could be replaced with any similar un-labelled SEM images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YLhiiLbWRXQO",
        "colab": {}
      },
      "source": [
        "# Check if .zip file exists, if not, download it from Google Drive\n",
        "if imgs_zip.exists():\n",
        "  print('Dataset already exists. Skipping download!')\n",
        "else:\n",
        "  print('Dataset does not exist... Downloading!')\n",
        "  !gdown --id $imgs_google_drive_id -O $imgs_zip\n",
        "\n",
        "# Unzip raw dataset\n",
        "!rm -rf $imgs_dir\n",
        "!unzip -o $imgs_zip -d $imgs_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfnP9zJNNj1L",
        "colab_type": "text"
      },
      "source": [
        "Now we will sort the input files which have many different magnifications into images that only have the desired magnification (50k in this case)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq1AXXKkMG11",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "in_files = list(imgs_dir.rglob('*.tif'))\n",
        "\n",
        "images = []\n",
        "# Start to loop over all TIFF files\n",
        "for file in in_files:\n",
        "    # Open each file using the TiffFile library\n",
        "    with tifffile.TiffFile(file) as tif:\n",
        "        \n",
        "        # Extract magnification data\n",
        "        mag = tif.sem_metadata['ap_mag'][1] \n",
        "        if type(mag) is str:  # Apply correction for \"k\" ex: mag = \"50 k\"\n",
        "            mag = float(mag.split(' ')[0]) * 1000\n",
        "        else:\n",
        "            mag = float(mag)\n",
        "\n",
        "        # Only filter the images that have the magnification that we are interested in\n",
        "        if not mag == desired_mag:\n",
        "          continue\n",
        "\n",
        "    images.append(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2KM5HQp4gHh",
        "colab_type": "text"
      },
      "source": [
        "Load a random image and show it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuFUcV5tq8Tv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "im_path = random.sample(images,1)[0]\n",
        "im = cv2.imread(str(im_path), cv2.IMREAD_GRAYSCALE)\n",
        "print(im.shape)\n",
        "cv2_imshow(im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szbPuPoKOKJR",
        "colab_type": "text"
      },
      "source": [
        "Do some pre-processing to get it ready to feed into our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYDE7vVMqq78",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model expects an RGB image, so copy the greyscale data into other 2 channels\n",
        "im_RGB = np.repeat(im[:, :, np.newaxis], 3, axis=2)\n",
        "print(im_RGB.shape)\n",
        "cv2_imshow(im_RGB)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDEx09lYVyYE",
        "colab_type": "text"
      },
      "source": [
        "## 3.2 - Load our model\n",
        "\n",
        "Now we will load a trained model and use it to label the above image. First we load a default config with `get_cfg()` and we then overwrite some of its parameters with our saved YAML configuration file. \n",
        "\n",
        "One important point is that we need to have `cfg.MODEL.WEIGHTS` set to point to the weights file. As this file can be quite big (>300MB) and since Github isn't designed to host big binary files, I have saved the weights for this model on my Google Drive instead. However, if you have your weights saved locally (ex: on your Google Drive), you can skip this download."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBQbpIgXTi9D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check if .zip file exists, if not, download it from Google Drive\n",
        "if raw_zip.exists():\n",
        "  print('Dataset already exists. Skipping download!')\n",
        "else:\n",
        "  print('Dataset does not exist... Downloading!')\n",
        "  !gdown --id $file_id -O $raw_zip\n",
        "\n",
        "# Unzip raw dataset\n",
        "!rm -rf $raw_dir\n",
        "!unzip -o $raw_zip -d $raw_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRf9lQoyeIGg",
        "colab_type": "text"
      },
      "source": [
        "Now we can go ahead with the rest of the configuration of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLjc8j5wY7Yr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_path)\n",
        "cfg.MODEL.WEIGHTS = str(weights_path)\n",
        "cfg.MODEL.DEVICE = 'cpu'  # CPU is enough for inference, no need for GPU\n",
        "\n",
        "# If we have a lot of objects to detect, need to set higher # of proposals here:\n",
        "cfg.MODEL.RPN.POST_NMS_TOPK_TEST = 1000\n",
        "cfg.MODEL.RPN.PRE_NMS_TOPK_TEST = 1000\n",
        "cfg.TEST.DETECTIONS_PER_IMAGE = 200\n",
        "\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5   # Set the testing threshold for this model\n",
        "cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = 0.2     # Non-max supression threshold\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(class_dict) # We have three classification classes \n",
        "\n",
        "# Setting allowed input sizes (avoid scaling)\n",
        "cfg.INPUT.MIN_SIZE_TEST = 0\n",
        "cfg.INPUT.MAX_SIZE_TEST = 99999\n",
        "\n",
        "\n",
        "# A bit of a hacky way to be able to use the DefaultPredictor:\n",
        "# Register a \"fake\" dataset to then set the 'thing_classes' metadata\n",
        "# (there is probably a better way to do this...)\n",
        "cfg.DATASETS.TEST = ('placeholder')\n",
        "DatasetCatalog.clear()\n",
        "DatasetCatalog.register(\"placeholder\", lambda _: None)\n",
        "MetadataCatalog.get(\"placeholder\").set(thing_classes=list(class_dict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xCA0YZyssvk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictor = DefaultPredictor(cfg)\n",
        "outputs = predictor(im_RGB)\n",
        "print('Number of detected objects = {}'.format(len(outputs[\"instances\"])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xE8ZPTc4yZAx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Verify outputs manually\n",
        "# outputs[\"instances\"].pred_classes\n",
        "# outputs[\"instances\"].pred_boxes\n",
        "# outputs[\"instances\"].scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLHfqmf1yesU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We can use Visualizer to draw the predictions on the image.\n",
        "v = Visualizer(im_RGB[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TEST[0]), scale=1.5)\n",
        "v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "cv2_imshow(v.get_image()[:, :, ::-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20XCitnL5F42",
        "colab_type": "text"
      },
      "source": [
        "## 3.4 - Post-processing model output\n",
        "\n",
        "However, just getting the output from the model isn't enough. Now we have to do bit more work to post-process the output and extract things like nanomembrane yield, sizes and other interesting data!\n",
        "\n",
        "First lets divide up the output of the neural net for further processing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGkdfktx4cME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cl = np.array(outputs[\"instances\"].pred_classes.cpu())  # Classes\n",
        "s = np.array(outputs[\"instances\"].scores.cpu()) # Prediction scores\n",
        "b =  np.array([x.numpy() for x in outputs[\"instances\"].pred_boxes])  # Bounding boxes\n",
        "c = np.array(outputs[\"instances\"].pred_boxes.get_centers())  # Bounding box centres\n",
        "m =  np.array([x.numpy() for x in outputs[\"instances\"].pred_masks])  # Segmentation masks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bUNMBtxEGhA",
        "colab_type": "text"
      },
      "source": [
        "Now we can loop over all the possible classes and display images with segmentation masks of each class individually."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sr_MzaJG5wQL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for c in range(len(class_dict)):\n",
        "  i_filt = list(np.argwhere(cl==c).flatten()) # Choose only the indixes with specific class\n",
        "\n",
        "  print(f\"{inv_class_dict[str(c+1)]}:\")\n",
        "\n",
        "  # We can use Visualizer to draw the predictions on the image.\n",
        "  v = Visualizer(im_RGB[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TEST[0]), scale=1.0)\n",
        "  v = v.draw_instance_predictions(outputs[\"instances\"][[i_filt]].to(\"cpu\"))\n",
        "  cv2_imshow(v.get_image()[:, :, ::-1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrog16S9Eboh",
        "colab_type": "text"
      },
      "source": [
        "Now we can start to mess around dimensional analysis. But first let's extract the pixel size from the raw TIF image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SZ1Fff0OS3v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tifffile.TiffFile(im_path) as tif:\n",
        "    \n",
        "    # Extract magnification data\n",
        "    mag = tif.sem_metadata['ap_mag'][1] \n",
        "    if type(mag) is str:  # Apply correction for \"k\" ex: mag = \"50 k\"\n",
        "        mag = float(mag.split(' ')[0]) * 1000\n",
        "    else:\n",
        "        mag = float(mag)\n",
        "\n",
        "    # Extract pixel size data\n",
        "    pixel_size = float(tif.sem_metadata['ap_pixel_size'][1])  # nm\n",
        "    if 'µm' in tif.sem_metadata['ap_pixel_size'][2]: # Correction for um\n",
        "        pixel_size *= 1000\n",
        "\n",
        "    # Extract tilt data\n",
        "    tilt = tif.sem_metadata['ap_tilt_angle'][1] # degrees\n",
        "    # tilt = tif.sem_metadata['ap_stage_at_t'][1]  # might be equivalent, not sure\n",
        "\n",
        "pixel_size_x = pixel_size  # nmd\n",
        "pixel_size_y = pixel_size / np.cos(np.deg2rad(tilt))  # nm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qC76PV5POVec",
        "colab_type": "text"
      },
      "source": [
        "Let's start with slit length/width."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pYmcv1AAk4p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i_filt = list(np.argwhere(cl+1==int(class_dict['slit'])).flatten()) # Choose only the indixes with specific class\n",
        "b_slits = b[i_filt]\n",
        "\n",
        "slit_widths = (b_slits[:,3] - b_slits[:,1]) * pixel_size_y\n",
        "slit_lengths = (b_slits[:,2] - b_slits[:,0]) * pixel_size_x\n",
        "\n",
        "print(f\"Mean slit width: {slit_widths.mean():.0f} +/- {slit_widths.std():.0f} nm\")\n",
        "print(f\"Mean slit length: {slit_lengths.mean():.0f} +/- {slit_lengths.std():.0f} nm\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}